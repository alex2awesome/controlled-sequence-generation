import torch
from operator import add
import torch.nn.functional as F

from generator.utils_methods import top_k_filter

SMALL_CONST = 1e-15
BIG_CONST = 1e10

def _get_gradient_mask(past, window_length, decay):
    """Generate a mask for gradient perturbation so that we only perturb gradients within the window."""
    decay_mask = 1.0 if not decay else torch.arange(0., 1.0 + SMALL_CONST, 1.0 / (window_length))[1:]

    # if version 3.0.2: (2, batch_size, num_heads, sequence_length, embed_size_per_head)
    # if version >4.0.0: ...
    curr_length = past[0].shape[3]
    if (curr_length > window_length) and (window_length > 0):
        ones_key_val_shape = tuple(past[0].shape[:-2]) + tuple([window_length]) + tuple(past[0].shape[-1:])
        zeros_key_val_shape = tuple(past[0].shape[:-2]) + tuple([curr_length - window_length]) + tuple(
            past[0].shape[-1:])
        #
        ones_mask = torch.ones(ones_key_val_shape)
        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)
        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)
        #
        return torch.cat((ones_mask, torch.zeros(zeros_key_val_shape)), dim=-2)
    else:
        return torch.ones_like(past[0])


def _compute_hidden_states(past, last, unpert_past, curr_perturbation, accumulated_hidden, lm, horizon_length):
    """Compute average embedding, based on perturbed past, over a horizon."""
    curr_length = curr_perturbation[0].shape[3]

    # perturb `past` (past_key_values) with the current stored gradient perturbation
    perturbed_past = list(map(add, past, curr_perturbation))
    # compute logits and hidden states from the language model
    all_logits, _, all_hidden = lm.get_lmhead_logits_and_past_and_hidden(input_ids=last, past_key_values=perturbed_past)
    # take the hidden state from the last layer of the language model
    hidden = all_hidden[-1]

    # add this hidden state to the accumulated hidden states
    new_accumulated_hidden = accumulated_hidden + torch.sum(hidden, dim=1).detach()
    logits = all_logits[:, -1, :] # all logits shape: (1, 1, 50257). logits.shape: (1, 50257)
    # word probabilities under the perturbed
    probs = F.softmax(logits, dim=-1)

    # perform horizon calculation. todo: check what the horizon really is -- do they mention in the paper?
    curr_unpert_past = unpert_past  # what is unperturbed past?
    curr_probs = torch.unsqueeze(probs, dim=1)  # shape: [1, 1, 50257]
    wte = lm.resize_token_embeddings()
    # take average embedding over a horizon
    for _ in range(horizon_length): # by default, horizon = 1
        input_embeds = torch.matmul(curr_probs, wte.weight.data)
        _, curr_unpert_past, curr_all_hidden = lm.get_lmhead_logits_and_past_and_hidden(
            past_key_values=curr_unpert_past,
            input_embeds=input_embeds
        )
        curr_hidden = curr_all_hidden[-1]
        new_accumulated_hidden += torch.sum(curr_hidden, dim=1)

    # normalize
    new_accumulated_hidden = new_accumulated_hidden / (curr_length + 1 + horizon_length)
    return probs, new_accumulated_hidden


def _calculate_loss(discrim_loss, unpert_logits, pert_probs, kl_scale):
    """
    Calculate:
        (1) discriminator loss, i.e. the CE loss of the discriminator on the peturbed vectors.
        (2) fluency loss, i.e. the KL divergence of the prob. vectors for the unpeturbed case and the peturbed case.
    """
    def _get_min_floor(p):
        # for all elements in probability vector p, make sure none are below a small constant.
        return SMALL_CONST * (p <= SMALL_CONST).float().detach()

    # kl loss
    unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)
    corrected_unpert_probs = unpert_probs + _get_min_floor(unpert_probs)
    corrected_pert_probs   = pert_probs   + _get_min_floor(pert_probs)
    kl_loss = (corrected_pert_probs * (corrected_pert_probs / corrected_unpert_probs).log()).sum()

    return discrim_loss, kl_loss, discrim_loss + (kl_scale * kl_loss)


def _norm_gradients(window_mask, curr_perturbation, current_stepsize, gamma):
    """Normalize gradients by calculating gradient norms, and then accumulate."""
    grads = []
    for p_ in curr_perturbation:
        grad_norm = torch.norm(p_.grad * window_mask) + SMALL_CONST
        # grad = -current_stepsize * (p_.grad * window_mask / max(1, grad_norm ** gamma)).data.cpu().numpy()
        grad = -current_stepsize * (p_.grad * window_mask / grad_norm ** gamma).data.cpu().numpy()
        grads.append(grad)
    return grads


def _fuse_model_predictions(unpert_logits, pert_logits, gm_scale, top_k):
    """Fuse the word-probabilites generated by the peturbed model with word probabilities generated by the unperturbed model."""
    pert_probs = F.softmax(pert_logits, dim=-1)

    # Fuse the modified model and original model
    unpert_probs = F.softmax(unpert_logits, dim=-1)
    pert_probs = ((pert_probs ** gm_scale) * (unpert_probs ** (1 - gm_scale)))  # + SMALL_CONST
    pert_probs = top_k_filter(pert_probs, k=top_k, probs=True)  # + SMALL_CONST

    # rescale
    if torch.sum(pert_probs) <= 1:
        pert_probs = pert_probs / torch.sum(pert_probs)

    return pert_probs


def _get_current_step_size(i, step_size, grad_length):
    """Check if we are above grad max length"""
    if i >= grad_length:
        current_stepsize = step_size * 0
    else:
        current_stepsize = step_size
    return current_stepsize


def clean_up(curr_perturbation, past):
    # reset gradients, just to make sure
    for p_ in curr_perturbation:
        p_.grad.data.zero_()

    # removing past from the graph
    new_past = []
    for p_ in past:
        new_past.append(p_.detach())
    return new_past

